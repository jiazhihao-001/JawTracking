import cv2
import mediapipe as mp
import numpy as np
import time
import pyk4a
from pyk4a import Config, PyK4A, ColorResolution, DepthMode, CalibrationType
import csv
from matplotlib import pyplot as plt
from mpl_toolkits import mplot3d  

# ファイルパスを決定する
filename = '.../data.csv'

# カメラ マトリクスを使用して 2D から 3D に移行する
def convert_2d_to_3d(u, v, z, K):
    v0 = K[1][2]
    u0 = K[0][2]
    fy = K[1][1]
    fx = K[0][0]
    x = int(-(u - u0) * z / fx)
    y = int(-(v - v0) * z / fy)
    return (x, y, int(z))

# HSVレンジ球状グリーン
green_light = np.array([20, 145, 60], np.uint8)
green_dark = np.array([179, 255, 255], np.uint8)

# データを保存するためにいくつかの配列を作成する
fpssource = []
distsource = []
center_points = []

MM_4_points = []
MM_171_points = []
MM_175_points = []
MM_8_points = []
MM_396_points = []

# Face mesh detection
mp_drawing = mp.solutions.drawing_utils
mp_face_mesh = mp.solutions.face_mesh

drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

with mp_face_mesh.FaceMesh(min_detection_confidence=0.5,min_tracking_confidence=0.5) as face_mesh:
    k4a = PyK4A(
        Config(
            color_resolution=pyk4a.ColorResolution.RES_720P,
            depth_mode=pyk4a.DepthMode.NFOV_UNBINNED,
        )
    )
    k4a.start()
    # 深度カメラの固有パラメータと歪み係数を取得する
    intrinsics_depth = k4a.calibration.get_camera_matrix(CalibrationType.DEPTH)
    distortion_depth = k4a.calibration.get_distortion_coefficients(CalibrationType.DEPTH)

    # カラーカメラの内部パラメータと歪み係数を取得
    intrinsics_color = k4a.calibration.get_camera_matrix(CalibrationType.COLOR)
    distortion_color = k4a.calibration.get_distortion_coefficients(CalibrationType.COLOR)
    cou = 0
    while True:
        prev_time = time.time()  # 開始時間を記録します
        capture = k4a.get_capture()
        if capture.color is not None and capture.transformed_depth is not None:
            color_height, color_width, _ = capture.color.shape
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
            image = capture.color
            # Flip the image horizontally for a later selfie-view display
            # Convert the BGR image to RGB
            # image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False
            # Process the image
            results = face_mesh.process(image)

            image.flags.writeable = True

            # Convert the image color back so it can be displayed
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            # 範囲
            # a = {149, 176, 140, 32, 194, 148, 171, 208, 201}
            # b = {378, 400, 369, 262, 418, 377, 396, 428, 421}
            # c = {200, 199, 175, 152, 4}

            a = {175}
            b = {171}
            c = {4, 8}
            d = {396}
            # c = list(range(100, 120))
            selected_points = set().union(a, b, c, d)
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
            xx, yy, hh, ww = 500, 240, 270, 270
            im = capture.color[yy:yy + hh, xx:xx + ww]

            # converting video streamed frames to HSV. which we will learn further
            hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)

            # Range Specification
            green = cv2.inRange(hsv, green_light, green_dark)

            green = cv2.GaussianBlur(green, (3, 3), 0)
            # Morphological Transformation using dilation
            kernel = np.ones((5, 5), "uint8")
            kernel_e = np.ones((5, 5), "uint8")
            green = cv2.erode(green, kernel_e)
            green = cv2.dilate(green, kernel)

            res_green = cv2.bitwise_and(im, im, mask=green)
            dep = cv2.applyColorMap(cv2.convertScaleAbs(capture.transformed_depth, alpha=0.03), cv2.COLORMAP_JET)

            # Tracking Green
            (contours, hierarchy) = cv2.findContours(green, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
            valid_targets = []
            for contour_index, contour in enumerate(contours):
                area = cv2.contourArea(contour)
                if area > 5:
                    (x, y), radius = cv2.minEnclosingCircle(contour)
                    x_relative = (int)(x)
                    y_relative = (int)(y)
                    depth_value = capture.transformed_depth[y_relative + yy, x_relative + xx]
                    xyz_curr = convert_2d_to_3d(x_relative + xx, y_relative + yy, depth_value, intrinsics_color)

                    valid_targets.append((x_relative, y_relative, xyz_curr[1], xyz_curr))
                    # cv2.circle(capture.color, center=(x_relative + xx, y_relative + yy), color=(100, 200, 255), thickness=2,
                    #            radius=1)
                    # cv2.circle(dep, center=(x_relative + xx, y_relative + yy), color=(0, 255, 255), thickness=2, radius=1)

            if valid_targets:
                valid_targets.sort(key=lambda x: x[2])
                target_x, target_y, _, target_xyz = valid_targets[0]
                cv2.circle(capture.color, center=(target_x + xx, target_y + yy), color=(100, 200, 255), thickness=2,
                           radius=1)
                # 追跡されるフレーム数を制限する
                # cou = cou + 1
                # if cou > 100 :
                #     break
                center_points.append(target_xyz)
            else:
                center_points.append((0, 0, 0))  # データが欠落している場所は 000 としてマークされます
            print(len(center_points))
            if len(center_points) > 0 :
                cv2.putText(capture.color, "Depth in mm: " + str(center_points[-1][2]), (250, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,
                            (0, 255, 0), 3)
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    for idx in selected_points:
                        if idx == 4:
                            point = face_landmarks.landmark[idx]
                            M_4_x = int(point.x * image.shape[1])
                            M_4_y = int(point.y * image.shape[0])
                            M_depth_value = capture.transformed_depth[M_4_y, M_4_x]
                            M_curr = convert_2d_to_3d(M_4_x, M_4_y, M_depth_value, intrinsics_color)
                            MM_4_points.append(M_curr)
                            # print(M_curr)
                            cv2.circle(capture.color, (M_4_x, M_4_y), 2, (0, 255, 0), -1)
                        if idx == 171:
                            point = face_landmarks.landmark[idx]
                            M_171_x = int(point.x * image.shape[1])
                            M_171_y = int(point.y * image.shape[0])
                            M_depth_value = capture.transformed_depth[M_171_y, M_171_x]
                            M_curr = convert_2d_to_3d(M_171_x, M_171_y, M_depth_value, intrinsics_color)
                            MM_171_points.append(M_curr)
                            # print(M_curr)
                            cv2.circle(capture.color, (M_171_x, M_171_y), 2, (0, 255, 0), -1)
                        if idx == 175:
                            point = face_landmarks.landmark[idx]
                            M_175_x = int(point.x * image.shape[1])
                            M_175_y = int(point.y * image.shape[0])
                            M_depth_value = capture.transformed_depth[M_175_y, M_175_x]
                            M_curr = convert_2d_to_3d(M_175_x, M_175_y, M_depth_value, intrinsics_color)
                            MM_175_points.append(M_curr)
                            # print(M_curr)
                            cv2.circle(capture.color, (M_175_x, M_175_y), 2, (0, 255, 0), -1)
                        if idx == 396:
                            point = face_landmarks.landmark[idx]
                            M_396_x = int(point.x * image.shape[1])
                            M_396_y = int(point.y * image.shape[0])
                            M_depth_value = capture.transformed_depth[M_396_y, M_396_x]
                            M_curr = convert_2d_to_3d(M_396_x, M_396_y, M_depth_value, intrinsics_color)
                            MM_396_points.append(M_curr)
                            cv2.circle(capture.color, (M_396_x, M_396_y), 2, (0, 255, 0), -1)
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
            TotalTime = time.time() - prev_time
            prev_time = time.time()
            if TotalTime > 0:
                fps = 1 / TotalTime
            else:
                fps = float('inf')
            if (fps < 250):
                cv2.putText(capture.color, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5,
                            (0, 255, 0), 2)
            else:
                cv2.putText(capture.color, f'FPS: 250', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
            side_length = min(color_width, color_height) // 8
            top_left_x = (color_width - side_length) // 2
            top_left_y = (color_height - side_length) // 2
            bottom_right_x = top_left_x + side_length
            bottom_right_y = top_left_y + side_length
            color = (0, 0, 255)  # 
            thickness = 2  

            cv2.rectangle(capture.color, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), color, thickness)

            cv2.imshow("Color", capture.color)
            # cv2.imshow("Transformed Depth", dep)
            # cv2.imshow("res_green Tracker", res_green)


        if cv2.waitKey(1) == ord(' ') & 0xFF != ord('q'):
            if len(center_points) > 0 and len(MM_4_points) > 0 and len(MM_175_points) > 0 and len(MM_396_points)> 0 and len(MM_171_points) > 0:
                print(len(center_points),len(MM_4_points),len(MM_175_points),len(MM_396_points), len(MM_171_points))
                result_data = []  # 最終結果を保存するリスト
                # 列ラベルを追加する
                header_row = ['center_point x', 'center_point y', 'center_point z', '4_point x', '4_point y',
                              '4_point z',
                              '175_point x', '175_point y', '175_point z', '396_point x',
                              '396_point y', '396_point z', '171_point x', '171_point y', '171_point z']
                result_data.append(header_row)

                max_length = max(len(center_points), len(MM_4_points), len(MM_175_points), len(MM_396_points),
                                 len(MM_171_points))
                for i in range(max_length):
                    row_data = []

                    if i < len(center_points):
                        center_point = center_points[i]
                        row_data.extend(center_point)
                    else:
                        row_data.extend([None, None, None])

                    if i < len(MM_4_points):
                        MM_4_point = MM_4_points[i]
                        row_data.extend(MM_4_point)
                    else:
                        row_data.extend([None, None, None])

                    if i < len(MM_175_points):
                        MM_175_point = MM_175_points[i]
                        row_data.extend(MM_175_point)
                    else:
                        row_data.extend([None, None, None])

                    if i < len(MM_396_points):
                        MM_396_point = MM_396_points[i]
                        row_data.extend(MM_396_point)
                    else:
                        row_data.extend([None, None, None])

                    if i < len(MM_171_points):
                        MM_171_point = MM_171_points[i]
                        row_data.extend(MM_171_point)
                    else:
                        row_data.extend([None, None, None])

                    result_data.append(row_data)

                with open(filename, 'w', newline='') as file:
                    writer = csv.writer(file)
                    writer.writerows(result_data)
            break
    k4a.stop()



